{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec444530-f8d0-4b33-b630-54bed8856f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "TOGETHER_API_KEY = os.getenv(\"8f9fd87ce52cce0f5c7d09bc2b4b9a34b18e2c0a3177544dc24a78751019af88\")\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://youtu.be/cdiD-9MMpb0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29f0a37-112f-4f8a-9d42-fdc671ec9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pinecone\n",
    "!pip install -U pinecone-client\n",
    "!pip install yt-dlp\n",
    "!pip install langchain-openai openai\n",
    "!pip install -U langchain\n",
    "!pip install langchain-community\n",
    "#8f9fd87ce52cce0f5c7d09bc2b4b9a34b18e2c0a3177544dc24a78751019af88\n",
    "!pip install yt-dlp\n",
    "!pip uninstall -y whisper\n",
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install yt-dlp transformers torchaudio\n",
    "!brew install ffmpeg\n",
    "!pip install sentence-transformers\n",
    "!pip install docarray\n",
    "!pip install langchain-pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17d3105-ab6b-452e-ba30-caef78afae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9k/p7wn_lg96lgdnmqlqpkn8vgr0000gn/T/ipykernel_45406/2194727865.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI(\n",
      "/var/folders/9k/p7wn_lg96lgdnmqlqpkn8vgr0000gn/T/ipykernel_45406/2194727865.py:11: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = model([HumanMessage(content=\"Who is putin?\")])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vladimir Putin is a Russian politician and former Russian military intelligence officer who has served as the President of the Russian Federation since 2000. He was previously the President of the Russian Federation from 1999 to 2000 and Prime Minister of the Russian Federation from 1998 to 1999. Putin is known for his tough stance on Russian politics and foreign policy, and has been involved in a number of high-profile military and diplomatic incidents during his time in office.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=\"8f9fd87ce52cce0f5c7d09bc2b4b9a34b18e2c0a3177544dc24a78751019af88\",\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "response = model([HumanMessage(content=\"Who is putin?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37a98f4-af02-4a1c-b20b-e45ae9d0ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_mKKXL_AnHxTu4jLNMcE5FTwCHYv8wpo73knLnAgAXUFbkGNYbTSMHtRTT2NgbrE6T6ibc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694470bb-5b5f-40b2-a4e3-b053f41223c3",
   "metadata": {},
   "source": [
    "OPENAI:\n",
    "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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a48ba-db1a-4578-a524-054d125fef91",
   "metadata": {},
   "source": [
    "PINECONE:\n",
    "\n",
    "pcsk_bQ4mj_316D2PpTtiMCMxEypJzMzeY71nEF6vxzvQcc6p9LrJUWtVDAG4dRTXfJNF96rcx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff0e6f3-d540-43cd-93e9-4d25beb4aa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' The Los Angeles Dodgers won the 2020 World Series during the COVID-19 pandemic.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 24, 'total_tokens': 47, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'mistralai/Mistral-7B-Instruct-v0.1', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--ae528cf9-3868-46e1-bb6f-aef8b9550476-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443ea5d7-7d03-4b26-9036-6000f16bbf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Los Angeles Dodgers won the World Series during the COVID-19 pandemic in 2020.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1222af7-7c37-4731-8d42-a768d41d8789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Mary\\'s sister is Susana\\n\\nQuestion: Who is Mary\\'s sister?\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b297052-f884-41f8-8a5e-61f6383f1e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Susana is Mary's sister.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Mary's sister is Susana\",\n",
    "    \"question\": \"Who is Mary's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad93a633-6c5c-4c4e-963f-ac8268859c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate {answer} to {language}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208f22ac-a62c-45d4-a66c-5cc7a1c16cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Mary has one sister, Susana. translates to \"Mary tiene una hermana, Susana\" in Spanish.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
    ")\n",
    "\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Mary's sister is Susana. She doesn't have any more siblings.\",\n",
    "        \"question\": \"How many sisters does Mary have?\",\n",
    "        \"language\": \"Spanish\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea0adb0-4229-4da6-bd19-f1f663ef0fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/whisper.py\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "print(whisper.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83bbf38-cd87-4fda-9423-17dfc28616ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n",
      "configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "libavutil      59. 39.100 / 59. 39.100\n",
      "libavcodec     61. 19.101 / 61. 19.101\n",
      "libavformat    61.  7.100 / 61.  7.100\n",
      "libavdevice    61.  3.100 / 61.  3.100\n",
      "libavfilter    10.  4.100 / 10.  4.100\n",
      "libswscale      8.  3.100 /  8.  3.100\n",
      "libswresample   5.  3.100 /  5.  3.100\n",
      "libpostproc    58.  3.100 / 58.  3.100\n",
      "ffprobe version 7.1.1 Copyright (c) 2007-2025 the FFmpeg developers\n",
      "built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n",
      "configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "libavutil      59. 39.100 / 59. 39.100\n",
      "libavcodec     61. 19.101 / 61. 19.101\n",
      "libavformat    61.  7.100 / 61.  7.100\n",
      "libavdevice    61.  3.100 / 61.  3.100\n",
      "libavfilter    10.  4.100 / 10.  4.100\n",
      "libswscale      8.  3.100 /  8.  3.100\n",
      "libswresample   5.  3.100 /  5.  3.100\n",
      "libpostproc    58.  3.100 / 58.  3.100\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -version\n",
    "!ffprobe -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8b4f8-3812-4738-9f07-0d245ecad3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import glob\n",
    "from transformers import pipeline\n",
    "import yt_dlp\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://youtu.be/cdiD-9MMpb0\"\n",
    "\n",
    "\n",
    "tmpdir = tempfile.mkdtemp()    #Creating temp directory\n",
    "\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': os.path.join(tmpdir, 'audio.%(ext)s'),   #Downloading audio using yt_dlp\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'quiet': True,\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([YOUTUBE_VIDEO])\n",
    "\n",
    "\n",
    "mp3_files = glob.glob(os.path.join(tmpdir, \"*.mp3\"))   #Locating the actual MP3 file\n",
    "if not mp3_files:\n",
    "    raise FileNotFoundError(\"No MP3 file found in temporary directory.\")\n",
    "audio_path = mp3_files[0]\n",
    "\n",
    "\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-base\",     #Transcribing using Whisper with timestamps and forced English output\n",
    "    model_kwargs={\"device_map\": \"auto\"},\n",
    ")\n",
    "result = transcriber(audio_path, return_timestamps=True, generate_kwargs={\"language\":\"en\", \"forced_decoder_ids\": None})\n",
    "\n",
    "\n",
    "transcription = \" \".join([chunk[\"text\"] for chunk in result[\"chunks\"]]).strip()   #Extracting plain transcription text\n",
    "\n",
    "\n",
    "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as f:    #Saving transcription to file\n",
    "    f.write(transcription)\n",
    "\n",
    "print(\"Transcription complete and saved to 'transcription.txt'.\")\n",
    "\n",
    "shutil.rmtree(tmpdir)                          #cleaning up\n",
    "model.generate(input_features=features, forced_decoder_ids=[[1, 50259], [2, 50359], [3, 50363]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bbd89-5940-44b1-8349-7946a80e9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58829db0-313c-4a27-9240-d8a885770f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"Is reading papers a good idea?\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4206d1d-4f75-493e-b8ed-6943b667ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb260805-d020-4329-aeec-d25bae71076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(text_documents)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9f6b9-5d39-4fc9-ae11-7bbd40a55a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d2935-4932-4256-9c8d-69cd9b7f42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embedded_query = embeddings.embed_query(\"Who is Mary's sister?\")\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa6a5a-c60d-4486-ac9b-e92ccae2ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = embeddings.embed_query(\"Mary's sister is Susana\")\n",
    "sentence2 = embeddings.embed_query(\"Pedro's mother is a teacher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf7c8e-d9f6-4832-8e00-c0223cfe617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    "\n",
    "query_sentence1_similarity, query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae1a23-7e17-4d53-a439-829d05ddeb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Mary's sister is Susana\",\n",
    "        \"John and Tommy are brothers\",\n",
    "        \"Patricia likes white cars\",\n",
    "        \"Pedro's mother is a teacher\",\n",
    "        \"Lucia drives an Audi\",\n",
    "        \"Mary has two siblings\",\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175bc07-3723-4979-86c4-0bfc8643fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore1.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58903a1c-36d4-4878-a70a-df96ef315c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever1 = vectorstore1.as_retriever()\n",
    "retriever1.invoke(\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8e85e-f5e6-4f7e-8773-1b722a4193b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
    "setup.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6b22b-f76c-4363-8c2e-1f605e38f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup | prompt | model | parser\n",
    "chain.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740d272-f8ec-46fc-8008-7560f91a58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What car does Lucia drive?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da5674-cc70-4d87-8800-34146df1a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9cf64-f921-409f-9e22-ef3404c9cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"What is synthetic intelligence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5962d-248d-40d6-9103-d92d61f4d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76703aa-668a-4701-8efc-19984e94c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone = Pinecone(api_key=\"pcsk_mKKXL_AnHxTu4jLNMcE5FTwCHYv8wpo73knLnAgAXUFbkGNYbTSMHtRTT2NgbrE6T6ibc\", environment=\"us-west1-gcp\")\n",
    "\n",
    "# Create index\n",
    "if \"youtube-rag-index\" not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=\"youtube-rag-index\",\n",
    "        dimension=1536,  # match this to your embedding size\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",         # or \"gcp\"\n",
    "            region=\"us-east-1\"   # or your region, e.g., \"us-east1\", \"us-central1\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8b57b-f185-41cd-a11a-f9068efacc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(\"youtube-rag-index\")\n",
    "vectorstore = PineconeVectorStore(index, embeddings, text_key=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d72bb-c741-4af9-93d3-0c02149283ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"What is Hollywood going to start doing?\")[:3]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd02c3-0657-4429-a9fa-665f46670938",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d87c36-a52b-4af6-bf74-6d04f5628d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"What is Hollywood going to start doing?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
